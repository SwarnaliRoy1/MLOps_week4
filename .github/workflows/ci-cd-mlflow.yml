name: CI + CD (DVC + MLflow + GAR + GKE)

on:
  push:
    branches: [ main, week6 ]
  pull_request:
    branches: [ main, week6 ]

permissions:
  contents: write
  pull-requests: write
  id-token: write

env:
  # ---- CI env ----
  MODEL_NAME: Iris-Classifier
  DATA_CSV_PATH: data/data.csv
  TARGET_COL: species
  # ---- CD env ----
  GAR_REGION: us-central1
  GAR_REPO: my-repo
  IMAGE_NAME: iris-api
  CLUSTER_NAME: gke-iris-cluster
  CLUSTER_LOCATION: us-central1
  NAMESPACE: default
  DOCKER_CONTEXT: ./fastapi            # <— your Dockerfile lives here
  GKE_DEPLOYMENT: iris-api             # <— k8s Deployment name

jobs:
  ci:
    name: Sanity (DVC + MLflow) + CML report
    runs-on: ubuntu-latest
    env:
      GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Set up CML
        uses: iterative/setup-cml@v2

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/req.txt') }}
          restore-keys: pip-${{ runner.os }}-

      - name: Install system deps
        run: |
          sudo apt-get update -y
          sudo apt-get install -y git-lfs
          git lfs install

      - name: Auth to GCP (for DVC GCS remote)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud (optional)
        if: ${{ env.GCP_PROJECT_ID != '' }}
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r req.txt

      - name: DVC pull data from GCS
        run: dvc pull -v

      - name: Resolve MLflow model URI (aliases -> latest)
        id: resolve
        env:
          MODEL_NAME: ${{ env.MODEL_NAME }}
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
        run: |
          python - <<'PY'
          import os, sys
          import mlflow
          from mlflow.tracking import MlflowClient
          from mlflow.exceptions import MlflowException

          mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
          name = os.environ["MODEL_NAME"]
          client = MlflowClient()

          try:
              client.get_registered_model(name)
          except Exception:
              print(f"::error::Registered model '{name}' not found.")
              sys.exit(1)

          uri = None
          for alias in ("champion","production"):
              try:
                  client.get_model_version_by_alias(name, alias)
                  uri = f"models:/{name}@{alias}"
                  print(f"Resolved via alias: {uri}")
                  break
              except MlflowException:
                  pass
          if uri is None:
              versions = client.search_model_versions(f"name='{name}'")
              if not versions:
                  print(f"::error::No versions for model '{name}'.")
                  sys.exit(1)
              latest = max(versions, key=lambda m: int(m.version))
              uri = f"models:/{name}/{latest.version}"
              print(f"Resolved via latest version: {uri}")

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"model_uri={uri}\n")
          PY

      - name: Evaluate model on CSV
        env:
          MODEL_URI: ${{ steps.resolve.outputs.model_uri }}
          DATA_CSV_PATH: ${{ env.DATA_CSV_PATH }}
          TARGET_COL: ${{ env.TARGET_COL }}
        run: |
          python - <<'PY'
          import os, json, pandas as pd, mlflow
          data_csv = os.environ["DATA_CSV_PATH"]
          target = os.environ["TARGET_COL"]
          model_uri = os.environ["MODEL_URI"]

          df = pd.read_csv(data_csv)
          X = df.drop(columns=[target])
          y = df[target]

          model = mlflow.pyfunc.load_model(model_uri)
          y_pred = model.predict(X)
          acc = float((y_pred == y).mean())
          report = {"model_uri": model_uri, "rows": int(len(df)), "accuracy": acc}
          print(json.dumps(report, indent=2))
          with open("sanity.json", "w") as f:
            json.dump(report, f, indent=2)
          PY

      - name: Publish CML report
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo '### CI Sanity (DVC + MLflow)' > report.md
          echo '' >> report.md
          echo '**Model URI:** `${{ steps.resolve.outputs.model_uri }}`' >> report.md
          echo '' >> report.md
          echo '\`\`\`json' >> report.md
          cat sanity.json >> report.md
          echo '\`\`\`' >> report.md
          cml comment create --publish report.md

  cd:
    name: CD to GKE (GAR build+push, deploy)
    runs-on: ubuntu-latest
    needs: ci
    # Run CD only on push to main or week6 branches (not PRs)
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/week6')
    env:
      GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Auth to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Configure Docker for Artifact Registry
        run: gcloud auth configure-docker $GAR_REGION-docker.pkg.dev -q

      - name: Build & push image to GAR
        run: |
          docker build -t ${IMAGE_NAME}:${{ github.sha }} ${DOCKER_CONTEXT}
          docker tag ${IMAGE_NAME}:${{ github.sha }} ${GAR_REGION}-docker.pkg.dev/${GCP_PROJECT_ID}/${GAR_REPO}/${IMAGE_NAME}:${{ github.sha }}
          docker tag ${IMAGE_NAME}:${{ github.sha }} ${GAR_REGION}-docker.pkg.dev/${GCP_PROJECT_ID}/${GAR_REPO}/${IMAGE_NAME}:latest
          docker push ${GAR_REGION}-docker.pkg.dev/${GCP_PROJECT_ID}/${GAR_REPO}/${IMAGE_NAME}:${{ github.sha }}
          docker push ${GAR_REGION}-docker.pkg.dev/${GCP_PROJECT_ID}/${GAR_REPO}/${IMAGE_NAME}:latest

      - name: Get GKE credentials
        uses: google-github-actions/get-gke-credentials@v2
        with:
          cluster_name: ${{ env.CLUSTER_NAME }}
          location: ${{ env.CLUSTER_LOCATION }}
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Deploy to GKE
        run: |
          # Inject PROJECT_ID into k8s image reference:
          sed -i "s/PROJECT_ID/${GCP_PROJECT_ID}/g" k8s/deployment.yaml
          kubectl apply -n ${NAMESPACE} -f k8s/deployment.yaml
          kubectl apply -n ${NAMESPACE} -f k8s/service.yaml
          kubectl rollout status deploy/${GKE_DEPLOYMENT} -n ${NAMESPACE} --timeout=180s

      - name: Wait for deployment availability
        run: |
          kubectl wait --for=condition=available deployment/${GKE_DEPLOYMENT} -n ${NAMESPACE} --timeout=180s

      - name: In-cluster smoke test (/)
        run: |
          kubectl run smoke --rm -i --restart=Never -n ${NAMESPACE} \
            --image=curlimages/curl -- \
            -sS http://${GKE_DEPLOYMENT}.${NAMESPACE}.svc.cluster.local/ \
            | tee smoke_root.txt
          grep -q 'Welcome to the Iris Classifier API' smoke_root.txt
