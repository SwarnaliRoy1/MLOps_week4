name: Sanity Test (DVC data + MLflow model)

on:
  push:
    branches: [ dev, main ]
  pull_request:
    branches: [ dev, main ]

permissions:
  contents: write
  pull-requests: write
  id-token: write

jobs:
  sanity:
    runs-on: ubuntu-latest
    env:
      MODEL_NAME: Iris-Classifier
      DATA_CSV_PATH: data/data.csv
      TARGET_COL: species
      GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
      # === CD env ===
      GAR_REGION: us-central1
      GAR_REPO: my-repo
      IMAGE_NAME: iris-api
      CLUSTER_NAME: gke-iris-cluster
      CLUSTER_LOCATION: us-central1    # regional cluster
      NAMESPACE: default

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Set up CML
        uses: iterative/setup-cml@v2

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/req.txt') }}
          restore-keys: pip-${{ runner.os }}-

      - name: Install system deps
        run: |
          sudo apt-get update -y
          sudo apt-get install -y git-lfs
          git lfs install

      - name: Auth to GCP (for DVC GCS remote)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud (optional)
        if: ${{ env.GCP_PROJECT_ID != '' }}
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r req.txt

      - name: DVC pull data from GCS
        run: dvc pull -v

      # Resolve MLflow model URI
      - name: Resolve MLflow model URI (aliases -> latest version)
        id: resolve
        env:
          MODEL_NAME: ${{ env.MODEL_NAME }}
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
        run: |
          python - <<'PY'
          import os, sys
          import mlflow
          from mlflow.tracking import MlflowClient
          from mlflow.exceptions import MlflowException

          mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
          name = os.environ["MODEL_NAME"]
          client = MlflowClient()

          def exists_registered_model(n):
            try:
              client.get_registered_model(n)
              return True
            except Exception:
              return False

          if not exists_registered_model(name):
            print(f"::error::Registered model '{name}' not found at {os.environ['MLFLOW_TRACKING_URI']}.")
            sys.exit(1)

          # Try aliases
          for alias in ("champion", "production"):
            try:
              v = client.get_model_version_by_alias(name, alias)
              uri = f"models:/{name}@{alias}"
              print(f"Resolved via alias: {uri} (version {v.version})")
              break
            except MlflowException:
              continue
          else:
            # Fallback to highest version
            versions = client.search_model_versions(f"name='{name}'")
            if not versions:
              print(f"::error::Model '{name}' has no versions.")
              sys.exit(1)
            latest = max(versions, key=lambda m: int(m.version))
            uri = f"models:/{name}/{latest.version}"
            print(f"Resolved via latest version fallback: {uri}")

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
            f.write(f"model_uri={uri}\n")
          PY

      - name: Evaluate model on CSV
        env:
          MODEL_URI: ${{ steps.resolve.outputs.model_uri }}
          DATA_CSV_PATH: ${{ env.DATA_CSV_PATH }}
          TARGET_COL: ${{ env.TARGET_COL }}
        run: |
          python - <<'PY'
          import os, json, pandas as pd, mlflow
          data_csv = os.environ["DATA_CSV_PATH"]
          target = os.environ["TARGET_COL"]
          model_uri = os.environ["MODEL_URI"]

          df = pd.read_csv(data_csv)
          X = df.drop(columns=[target])
          y = df[target]

          model = mlflow.pyfunc.load_model(model_uri)
          y_pred = model.predict(X)
          acc = float((y_pred == y).mean())

          report = {"model_uri": model_uri, "rows": int(len(df)), "accuracy": acc}
          print(json.dumps(report, indent=2))
          with open("sanity.json", "w") as f:
            json.dump(report, f, indent=2)
          PY

      - name: Create and publish CML report
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo '### Sanity Check (DVC data + MLflow model)' > report.md
          echo '' >> report.md
          echo '**Model URI:** `${{ steps.resolve.outputs.model_uri }}`' >> report.md
          echo '' >> report.md
          echo '\`\`\`json' >> report.md
          cat sanity.json >> report.md
          echo '\`\`\`' >> report.md
          cml comment create --publish report.md

      # ===================== CD: build, push, deploy (push to main only) =====================
      - name: Configure Docker for Artifact Registry
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: gcloud auth configure-docker $GAR_REGION-docker.pkg.dev -q

      - name: Build & push Docker image to GAR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          docker build -t ${IMAGE_NAME}:${{ github.sha }} .
          docker tag ${IMAGE_NAME}:${{ github.sha }} ${GAR_REGION}-docker.pkg.dev/${GCP_PROJECT_ID}/${GAR_REPO}/${IMAGE_NAME}:${{ github.sha }}
          docker tag ${IMAGE_NAME}:${{ github.sha }} ${GAR_REGION}-docker.pkg.dev/${GCP_PROJECT_ID}/${GAR_REPO}/${IMAGE_NAME}:latest
          docker push ${GAR_REGION}-docker.pkg.dev/${GCP_PROJECT_ID}/${GAR_REPO}/${IMAGE_NAME}:${{ github.sha }}
          docker push ${GAR_REGION}-docker.pkg.dev/${GCP_PROJECT_ID}/${GAR_REPO}/${IMAGE_NAME}:latest

      - name: Get GKE credentials
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: google-github-actions/get-gke-credentials@v2
        with:
          cluster_name: ${{ env.CLUSTER_NAME }}
          location: ${{ env.CLUSTER_LOCATION }}
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Deploy to GKE
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          # Substitute PROJECT_ID in deployment image
          sed -i "s/PROJECT_ID/${GCP_PROJECT_ID}/g" k8s/deployment.yaml
          kubectl apply -n ${NAMESPACE} -f k8s/deployment.yaml
          kubectl apply -n ${NAMESPACE} -f k8s/service.yaml
          kubectl rollout status deploy/iris-api -n ${NAMESPACE} --timeout=180s

      - name: Wait for pods to be Ready
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          kubectl wait --for=condition=available deployment/iris-api -n ${NAMESPACE} --timeout=180s

      - name: In-cluster smoke test (/healthz)
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          # Run a temporary pod that curls the Service inside the cluster
          kubectl run smoke --rm -i --restart=Never -n ${NAMESPACE} \
            --image=curlimages/curl -- \
            -sS http://iris-api.${NAMESPACE}.svc.cluster.local/healthz \
            | tee smoke_health.txt
          grep -q '"status":"ok"' smoke_health.txt
